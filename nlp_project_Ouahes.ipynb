{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = self.word2vec.keys()\n",
    "        self.id2word = [k for k in self.word2id]\n",
    "        self.embeddings = np.array([vec for vec in self.word2vec.values()], ndmin = 2)\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        scorelist=[]\n",
    "        #size = len(self.id2word)\n",
    "        #count = 0\n",
    "        for wo in list(self.id2word):\n",
    "            scorelist.append(self.score(w,wo))\n",
    "            #count = count + 1\n",
    "            #if count%1000 == 0:\n",
    "            #    print(str(count)+'/'+str(size)+' processed')\n",
    "        #idxs=sorted(scorelist)[::-1][:K+1]\n",
    "        idxs=np.argsort(scorelist)[::-1][:K+1]\n",
    "        most_similar_word = []\n",
    "        for i in idxs:\n",
    "            most_similar_word.append(list(self.word2vec.keys())[i])\n",
    "        return most_similar_word\n",
    "        \n",
    "        return idxs\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        idx1=list(self.id2word).index(w1)\n",
    "        idx2=list(self.id2word).index(w2)\n",
    "        v1=self.embeddings[idx1]\n",
    "        v2=self.embeddings[idx2] \n",
    "        score=np.dot(v1,v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2))  # dot-product of normalized vector = cosine similarity\n",
    "        return  score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052403\n",
      "paris france 0.7775108541288561\n",
      "germany berlin 0.7420295235998392\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "['dogs', 'dog', 'Dogs', 'doggies', 'canines', 'puppies']\n",
      "['paris', 'france', 'Paris', 'london', 'berlin', 'europe']\n",
      "['germany', 'europe', 'german', 'berlin', 'france', 'italy']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=55000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    \n",
    "    def encode(self,sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        self.sentemb = []\n",
    "        # self.sentences = sentences\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                mv=np.mean([w2v.word2vec[w] if w in w2v.word2vec else np.zeros((300,))for w in sent ], axis=0)\n",
    "                self.sentemb.append(mv)\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                mv=np.mean([w2v.word2vec[w]*idf[w] if w in w2v.word2vec else np.zeros((300,))for w in sent ], axis=0)\n",
    "                self.sentemb.append(mv)\n",
    "        \n",
    "        return np.vstack(self.sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences)\n",
    "        query = self.encode([s])\n",
    "        idx_s =sentences.index(s)\n",
    "        similarities = keys*query/np.linalg.norm(keys)/np.sqrt(np.sum(keys**2, axis = 0))\n",
    "        similarities = np.sum(similarities, axis = 1)\n",
    "        idxs=np.argsort(similarities)[-K:]\n",
    "        # return [sentences[i] for i in np.argsort(similarities)[-K:]]\n",
    "        print('\\n Top-%s similar sentences of \\n\"%s\" : \\n\\n' % (K, ' '.join(sentences[idx_s])))\n",
    "        for i, idx in enumerate(idxs):\n",
    "            print('%s) %s' % (i + 1, ' '.join(sentences[idx])))\n",
    "        \n",
    "        \n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        sentence_emb = self.encode(sentences)\n",
    "        idx1 = sentences.index(s1)\n",
    "        idx2 = sentences.index(s2)\n",
    "        v1 = sentence_emb[idx1]\n",
    "        v2 = sentence_emb[idx2]\n",
    "        score = np.dot(v1, v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "        print('\\n The score between \"%s\" and \"%s\" is %s' %(' '.join(sentences[idx1]),' '.join(sentences[idx2]), score))\n",
    "    \n",
    "  \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf={}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w]= max(1,np.log10(len(sentences) / (idf.get(w, 0) + 1)))\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "\n",
      " Top-5 similar sentences of \n",
      "\"1 smiling african american boy .\" : \n",
      "\n",
      "\n",
      "1) a boy jumps on another boy .\n",
      "2) a boy skateboarding\n",
      "3) teen boy playing billiards .\n",
      "4) boy riding a horse .\n",
      "5) boy plays baseball .\n",
      "\n",
      " The score between \"1 man singing and 1 man playing a saxophone in a concert .\" and \"10 people venture out to go crosscountry skiing .\" is 0.6089445116147131\n",
      "\n",
      " Top-5 similar sentences of \n",
      "\"1 smiling african american boy .\" : \n",
      "\n",
      "\n",
      "1) a boy jumps on another boy .\n",
      "2) a boy skateboarding\n",
      "3) teen boy playing billiards .\n",
      "4) boy riding a horse .\n",
      "5) boy plays baseball .\n",
      "\n",
      " The score between \"1 man singing and 1 man playing a saxophone in a concert .\" and \"10 people venture out to go crosscountry skiing .\" is 0.6089445116147131\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "with open('data/sentences.txt', 'r', encoding = 'utf-8') as file:\n",
    "    for line in file:\n",
    "        sentences += [line.split(' ')[:-1]]# we get rid of the \\n\n",
    "file.close()\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if True else s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "\n",
    "idf = {}  \n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49999 pretrained word vectors\n",
      "Loaded 49999 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "def load_data_vec(url, max_count, output_file):\n",
    "    r = requests.get(url, stream = True)\n",
    "    data = {}\n",
    "    counter =0\n",
    "    with open(output_file, 'w', encoding = \"utf-8\") as output_file:\n",
    "        for line in r.iter_lines():\n",
    "            if counter >= max_count:\n",
    "                break\n",
    "            splits = str(line).split(\" \")\n",
    "            if len(splits) == 302:\n",
    "                counter += 1\n",
    "                line = str(line).replace(\"b'\", \"\").replace(\"'b\", \"\")\n",
    "            \n",
    "                output_file.write(str(line)+ \"\\n\")\n",
    "    output_file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "english_vec_url = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\"\n",
    "french_vec_url = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\"\n",
    "download = False\n",
    "if download : french_vec = load_data_vec(french_vec_url, 50000, os.path.join(PATH_TO_DATA, \"french.vec\"))\n",
    "if download : english_vec = load_data_vec(english_vec_url, 50000, os.path.join(PATH_TO_DATA, \"english.vec\"))\n",
    "    \n",
    "w2vfrench = Word2vec(os.path.join(PATH_TO_DATA, 'french.vec'), nmax=50000)\n",
    "w2venglish = Word2vec(os.path.join(PATH_TO_DATA, 'english.vec'), nmax=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "english_list =[]\n",
    "french_list = []\n",
    "\n",
    "english_list=list(w2venglish.word2id)\n",
    "french_list=list(w2vfrench.word2id)\n",
    "common_words = set(list(w2venglish.word2id))& set(list(w2vfrench.word2id))\n",
    "common_words = [ x for x in iter(common_words)]\n",
    "\n",
    "\n",
    "X_matrix = []\n",
    "Y_matrix = []\n",
    "for i in range(len(common_words)):\n",
    "    X_matrix.append(w2vfrench.word2vec[common_words[i]])\n",
    "    Y_matrix.append(w2venglish.word2vec[common_words[i]])\n",
    "    \n",
    "# return training matrices\n",
    "X=np.array(X_matrix).T\n",
    "Y=np.array(Y_matrix).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "\n",
    "U,s,V = np.linalg.svd(np.dot(Y,X.transpose()))\n",
    "W = np.dot(U,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 closest translations of \n",
      "\"cat\" are \n",
      "\n",
      "\n",
      "1) grizzly\n",
      "2) chat\n",
      "3) felis\n",
      "4) canis\n",
      "5) cat\n",
      "Top-5 closest translations of \n",
      "\"dog\" are \n",
      "\n",
      "\n",
      "1) pig\n",
      "2) dingo\n",
      "3) chien\n",
      "4) dog\n",
      "5) hound\n",
      "Top-5 closest translations of \n",
      "\"queen\" are \n",
      "\n",
      "\n",
      "1) princess\n",
      "2) consort\n",
      "3) reine\n",
      "4) queen\n",
      "5) \\xe2\\x94\\x9c\\xe2\\x94\\x80\n",
      "Top-5 closest translations of \n",
      "\"boy\" are \n",
      "\n",
      "\n",
      "1) daughter\n",
      "2) dogg\n",
      "3) sailor\n",
      "4) girl\n",
      "5) boy\n",
      "Top-5 closest translations of \n",
      "\"language\" are \n",
      "\n",
      "\n",
      "1) parl\\xc3\\xa9e\n",
      "2) linguistics\n",
      "3) linguistic\n",
      "4) language\n",
      "5) languages\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "def translation(word,source_matrix,translation_matrix, target_matrix, K = 5):\n",
    "    target_matrix.embeddings=np.array(target_matrix.embeddings, ndmin=2)\n",
    "    # create the vector source of the word we want to translate\n",
    "    vector_source = source_matrix.word2vec[word].reshape(-1,)\n",
    "    # translate with the translation matrix the vector source\n",
    "    vector_target = np.dot(translation_matrix.T, vector_source).reshape(-1,)\n",
    "    \n",
    "    similarities = vector_target.T * target_matrix.embeddings/ np.linalg.norm(vector_target)/np.sqrt(np.sum(target_matrix.embeddings**2, axis = 0))\n",
    "    similarities = np.sum(similarities, axis = 1)\n",
    "    \n",
    "    idxs=np.argsort(similarities)[-K:]\n",
    "\n",
    "    print('Top-%s closest translations of \\n\"%s\" are \\n\\n' % (K, word))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        print('%s) %s' % (i + 1, target_matrix.id2word[idx]))\n",
    "        \n",
    "wordlist = ['cat', 'dog','queen','boy','language']\n",
    "for wordtotranslate in wordlist:\n",
    "    translation(wordtotranslate,w2venglish, W, w2vfrench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "def load_sentences(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences += [line.split(' ')[:-1]]# we get rid of the \\n\n",
    "    file.close()\n",
    "    return sentences\n",
    "\n",
    "test = load_sentences(os.path.join(PATH_TO_DATA, \"stsa.fine.test.X\"))\n",
    "dev = load_sentences(os.path.join(PATH_TO_DATA, \"stsa.fine.dev\"))\n",
    "train = load_sentences(os.path.join(PATH_TO_DATA, \"stsa.fine.train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "s2v = BoV(w2v)\n",
    "#build idf\n",
    "train_idf=s2v.build_idf(train)\n",
    "dev_idf=s2v.build_idf(dev)\n",
    "test_idf=s2v.build_idf(test)\n",
    "\n",
    "#defining labels\n",
    "ydev=[int(x[0]) for x in dev]\n",
    "ytrain=[int(x[0]) for x in train]\n",
    "xtrain=[x[1:] for x in train]\n",
    "xdev=[x[1:] for x in dev]\n",
    "\n",
    "#encode training set sentences weighted average\n",
    "Xtrain_idf = s2v.encode(xtrain, train_idf)\n",
    "Xdev_idf = s2v.encode(xdev, dev_idf)\n",
    "Xtest_idf = s2v.encode(test, test_idf)\n",
    "\n",
    "#encode training set sentences mean average word vector\n",
    "Xtrain = s2v.encode(xtrain)\n",
    "Xdev = s2v.encode(xdev)\n",
    "Xtest = s2v.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best valid score: 0.37693006357856496\n",
      "Best L2-reg parameter: 0.5\n",
      "=================================== best parameters ===============================\n",
      " best prameter for\n",
      "1.25\n",
      "======================================score devset=================================\n",
      "0.37693006357856496\n",
      "=================================== best parameters ===============================\n",
      " best prameter for\n",
      "1.25\n",
      "======================================score devset=================================\n",
      "0.368755676657584\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "bestreg_val_score=0\n",
    "best_score=0\n",
    "for reg in [2**t for t in range(-2, 4, 1)]:\n",
    "    clf = LogisticRegression(C=reg, random_state=1234)\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    if clf.score(Xdev, ydev)>best_score:\n",
    "        best_score=clf.score(Xdev,ydev)\n",
    "        bestreg_val_score = reg\n",
    "print('Best valid score:',best_score)\n",
    "print('Best L2-reg parameter:',bestreg_val_score)\n",
    "\n",
    "#grid search for optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def optiGrid (x,y,xdev,ydev):\n",
    "    parameters = {'C':[1, 1.2, 1.25, 1.3, 1.4, 1.5, 1.6, 1.7,1.8] }\n",
    "    gs = GridSearchCV(LogisticRegression(), parameters)\n",
    "    gs.fit(Xtrain, ytrain)\n",
    "    p=list(gs.best_params_.values())[0]\n",
    "    clf = LogisticRegression(C=p, random_state=1234)\n",
    "    model=clf.fit(x, y)\n",
    "    score=model.score(xdev, ydev)\n",
    "    \n",
    "    print(\"=================================== best parameters ===============================\")\n",
    "    print(\" best prameter for\".format(x,y))\n",
    "    print(p)\n",
    "    print(\"======================================score devset=================================\")\n",
    "    print(score)\n",
    "\n",
    "optiGrid(Xtrain,ytrain,Xdev, ydev)\n",
    "optiGrid(Xtrain_idf,ytrain,Xdev_idf, ydev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "\n",
    "\n",
    "clf = LogisticRegression(C=1.25, random_state=1234)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "pred=clf.predict(Xtest)\n",
    "\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "np.savetxt(os.path.join(PATH_TO_DATA, \"logreg_bov_y_test_sst.txt\"),pred,fmt='%s',newline=os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================================================\n",
      "Testing RandomForestClassifier\n",
      "Learing time 1.5050480365753174s\n",
      "Predicting time 0.0s\n",
      "=================================== Results ======================================\n",
      " score0.29155313351498635\n",
      "\n",
      "==================================================================================\n",
      "Testing GradientBoostingClassifier\n",
      "Learing time 148.03811192512512s\n",
      "Predicting time 0.03125119209289551s\n",
      "=================================== Results ======================================\n",
      " score0.3723887375113533\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "import time\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "#fonction classifier\n",
    "def test_classifier(X_train, y_train, X_dev, y_dev, classifier):\n",
    "    print(\"\")\n",
    "    print(\"==================================================================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    print(\"Testing \" + classifier_name)\n",
    "    now = time.time()\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    print(\"Learing time {0}s\".format(time.time() - now))\n",
    "    now = time.time()\n",
    "    s=model.score(X_dev, y_dev)\n",
    "    print(\"Predicting time {0}s\".format(time.time() - now))\n",
    "\n",
    "    print(\"=================================== Results ======================================\")\n",
    "    print(\" score \" + str(s))\n",
    "\n",
    "#multiple model\n",
    "rf = RandomForestClassifier()\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "test_classifier(Xtrain, ytrain, Xdev, ydev, rf)\n",
    "test_classifier(Xtrain, ytrain, Xdev, ydev,gbm)\n",
    "\n",
    "#grid search\n",
    "def optiGridGB (x,y):\n",
    "    gb_grid_params = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [4, 6, 8],\n",
    "              'min_samples_leaf': [20, 50,100,150],\n",
    "              #'max_features': [1.0, 0.3, 0.1] \n",
    "              }\n",
    "\n",
    "    gb = GradientBoostingClassifier(n_estimators = 600)\n",
    "\n",
    "    clf =GridSearchCV(gb,gb_grid_params,cv=2)\n",
    "    clf.fit(x, y)\n",
    "\n",
    "  \n",
    "    \n",
    "    print(\"=================================== best parameters ===============================\")\n",
    "    print(\" best prameter for\".format(x,y))\n",
    "    print(clf.best_params_)\n",
    "\n",
    "\n",
    "#optiGridGB(Xtrain,ytrain)\n",
    "#optiGridGB(Xtrain_w,ytrain)\n",
    "\n",
    "# Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "#results gradient boosting used\n",
    "clf = GradientBoostingClassifier(learning_rate=0.1, n_estimators=600, subsample=0.8)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "pred_gb=clf.predict(Xtest)\n",
    "\n",
    "np.savetxt(os.path.join(PATH_TO_DATA, \"gb_bov_y_test_sst.txt\"),pred_gb,fmt='%s',newline=os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "model=clf.fit(Xtrain, ytrain)\n",
    "pred_gb=model.predict(Xtest)\n",
    "\n",
    "np.savetxt(os.path.join(PATH_TO_DATA, \"gb_bov_y_test_sst.txt\"),pred_gb,fmt='%s',newline=os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "K.clear_session()\n",
    "\n",
    "def load_sentences_txt(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            sentences += [str(line).replace(\"'b\", \"\")\n",
    "                          .replace(\"\\n\", \"\")[2:]]\n",
    "    input_file.close()\n",
    "    return sentences\n",
    "\n",
    "\n",
    "PATH_TO_DATA = \"data/\"\n",
    "#64 sentences for one batch\n",
    "test_lstm = load_sentences_txt(os.path.join(PATH_TO_DATA, \"stsa.fine.test.X\"))\n",
    "dev_lstm = load_sentences_txt(os.path.join(PATH_TO_DATA, \"stsa.fine.dev\"))\n",
    "train_lstm = load_sentences_txt(os.path.join(PATH_TO_DATA, \"stsa.fine.train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17613\n",
      "[3949, 2287, 17879, 18817, 1022, 6093, 16608, 5050, 18709, 7692, 18817, 18326, 13693, 18817, 4322, 4511, 7283]\n",
      "[18148, 19132, 17537, 19455, 12624, 18709, 12784]\n",
      "[883, 11977, 5214, 20857, 12659, 18326, 18778, 5045, 15781, 3949, 6117, 2765, 17224, 7255, 517, 13861, 21007]\n"
     ]
    }
   ],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "vocab=[]\n",
    "for sent in train_lstm :\n",
    "    for word in sent.split(' '):\n",
    "            vocab.append(word)\n",
    "for sent in dev_lstm :\n",
    "    for word in sent.split(' '):\n",
    "            vocab.append(word)\n",
    "            \n",
    "\n",
    "#vocab size\n",
    "vocab_size= len(set(vocab))\n",
    "print(vocab_size)\n",
    "\n",
    "# integer encode the document\n",
    "train_encode = [one_hot(sent, round(vocab_size*1.3))for sent in train_lstm]\n",
    "test_encode = [one_hot(sent, round(vocab_size*1.3))for sent in test_lstm]\n",
    "dev_encode = [one_hot(sent, round(vocab_size*1.3))for sent in dev_lstm]\n",
    "\n",
    "print(train_encode[0])\n",
    "print(test_encode[0])\n",
    "print(dev_encode[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "X_train = sequence.pad_sequences(train_encode)\n",
    "maxseqlen=X_train.shape[1]\n",
    "X_train = sequence.pad_sequences(train_encode, maxlen=maxseqlen)\n",
    "X_val = sequence.pad_sequences(dev_encode, maxlen=maxseqlen)\n",
    "X_test = sequence.pad_sequences(test_encode, maxlen=maxseqlen)\n",
    "\n",
    "#labels in dummies\n",
    "y_val=to_categorical(ydev)\n",
    "y_train=to_categorical(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n",
      "C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 0  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "#adapted\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = len(vocab)  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))#use softmax instead of sigmoid for a better accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          5914784   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 5,939,941\n",
      "Trainable params: 5,939,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "loss_classif     =  'binary_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/4\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.2452 - acc: 0.8996 - val_loss: 0.5774 - val_acc: 0.7697\n",
      "Epoch 2/4\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.1932 - acc: 0.9243 - val_loss: 0.6496 - val_acc: 0.7651\n",
      "Epoch 3/4\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.1512 - acc: 0.9438 - val_loss: 0.7032 - val_acc: 0.7633\n",
      "Epoch 4/4\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.1249 - acc: 0.9548 - val_loss: 0.8058 - val_acc: 0.7557\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1702f4ce0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x170323535c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 4\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "#Plotting the evolution of train/dev results w.r.t the number of epochs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101/1101 [==============================] - 0s 322us/step\n",
      "\n",
      "acc: 75.57%\n"
     ]
    }
   ],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_val, y_val)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "prediction=model.predict(X_test)\n",
    "np.savetxt(os.path.join(PATH_TO_DATA, \"logreg_lstm_y_test_sst.txt\"),prediction,fmt='%.18g',newline=os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[56,41] = 19197 is not in [0, 17613)\n\t [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\n\nCaused by op 'embedding_1/Gather', defined at:\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-85-81069fbe0029>\", line 34, in <module>\n    model2.add(Embedding(vocab_size, embed_dim))\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 467, in add\n    layer(x)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1211, in gather\n    return tf.gather(reference, indices)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2585, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2334, in gather\n    validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[56,41] = 19197 is not in [0, 17613)\n\t [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[56,41] = 19197 is not in [0, 17613)\n\t [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-81069fbe0029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m           validation_data=(X_val, y_val))\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[56,41] = 19197 is not in [0, 17613)\n\t [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\n\nCaused by op 'embedding_1/Gather', defined at:\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-85-81069fbe0029>\", line 34, in <module>\n    model2.add(Embedding(vocab_size, embed_dim))\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 467, in add\n    layer(x)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1211, in gather\n    return tf.gather(reference, indices)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2585, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2334, in gather\n    validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\izaou\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[56,41] = 19197 is not in [0, 17613)\n\t [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "\n",
    "K.clear_session()\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "\n",
    "vocab=[]\n",
    "for sent in train_lstm :\n",
    "    for word in sent.split(' '):\n",
    "            vocab.append(word)\n",
    "for sent in dev_lstm :\n",
    "    for word in sent.split(' '):\n",
    "            vocab.append(word)\n",
    "            \n",
    "vocab_size = len(set(vocab))  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "pool_size = 4\n",
    "# create the model\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embed_dim))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model2.add(MaxPooling1D(pool_size=pool_size))\n",
    "model2.add(LSTM(nhid))\n",
    "model2.add(Dense(5, activation= 'softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model2.fit(X_train, y_train,\n",
    "          batch_size=bs,\n",
    "          epochs=n_epochs,\n",
    "          validation_data=(X_val, y_val))\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "scores = model2.evaluate(X_val, y_val)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "np.savetxt(os.path.join(PATH_TO_DATA, \"innovate_y_test_sst.txt\"),pred,fmt='%s',newline=os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
